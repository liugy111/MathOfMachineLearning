[TOC]

# Overview

1. multi-task learning æ˜¯ lifelong learning çš„ upper bound, ç”¨ multi-task learning çš„æ–¹æ³•å°±å¯ä»¥è§£å†³æ¨¡å‹é—å¿˜çš„é—®é¢˜
2. ç»ˆèº«å­¦ä¹ è¢«æå®æ¯…æ€»ç»“ä¸ºä¸‰å¤§æ–¹å‘ï¼šknowledge retention, knowledge transfer, model expansion

# Lifelong Learning Performance Evaluation

![](imgs/3.png)

# 1. Knowledge Retention: to memorize previous tasks

## (1) Elastic Weight Consolidation (EWC) 

åœºæ™¯ï¼šç»ˆèº«å­¦ä¹ ï¼ˆå­¦äº†ä¸€ä¸ªä»»åŠ¡ä¹‹ååˆè¦åœ¨å½“å‰ç½‘ç»œå­¦ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼‰

æˆ‘ä»¬å¯ä»¥è‚¯å®šçš„æ˜¯å¯¹äºç‰¹å®šçš„ä»»åŠ¡ï¼Œç½‘ç»œä¸­æœ‰äº›å‚æ•°é‡è¦ï¼Œæœ‰äº›ä¸é‡è¦ï¼ˆè¿™ä¹Ÿæ˜¯ç½‘ç»œå‹ç¼©çš„åŠ¨æœºä¹‹ä¸€ï¼‰ã€‚

EWCçš„æ–¹æ³•æ˜¯ï¼šEach parameter $\theta_i^b$ has a â€œguardâ€  $b_i$, å‘Šè¯‰æˆ‘ä»¬è¿™ä¸ªå‚æ•°æœ‰å¤šé‡è¦ï¼Œç±»ä¼¼äºç½‘ç»œå‹ç¼©ç®—æ³•ä¸­çš„ saliency scoreã€‚

EWCæŸå¤±å‡½æ•°ä¸­ï¼Œç›¸å½“äºåŠ äº†ä¸€é¡¹L2èŒƒæ•°ã€‚

![](imgs/0.png)

> If $ğ‘_ğ‘– = 0$, there is no constraint on $ğœƒ_ğ‘–$
>
> If $ğ‘_ğ‘– = \infty$, $ğœƒ_ğ‘–$ would always be equal to $ğœƒ_ğ‘–^ğ‘$

ç ”ç©¶çš„æ–¹å‘æ˜¯å¦‚ä½•è®¾è®¡ $b_i$ çš„å€¼ã€‚

### ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼šäºŒé˜¶å¾®åˆ†

![](imgs/1.png)

### å…¶å®ƒæ–¹æ³• (todo)

1. Synaptic Intelligence (SI), ICML 2017

   > https://arxiv.org/abs/1703.04200

2. Memory Aware Synapses (MAS), ECCV 2018

   > Special part: Do not need labelled data
   > https://arxiv.org/abs/1711.09601

## (2) Generating Data

### Motivation

å¦‚æœä¸€ä¸ªæ¨¡å‹åšä¸åˆ° lifelong learningï¼ˆä¹Ÿå°±æ˜¯è¯´å®ƒä¸èƒ½è®°ä½è¿‡å»åŠ è½½è¿‡çš„èµ„æ–™ï¼‰ï¼Œå¯ä¸å¯ä»¥è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼ˆtrain a generatorï¼‰ï¼Œä½¿å…¶èƒ½ç”Ÿæˆï¼ˆgenerateï¼‰è¿‡å»çš„èµ„æ–™ï¼Œè€Œä¸æ˜¯éœ€è¦é‡æ–°åŠ è½½è¿‡å»çš„èµ„æ–™ï¼ˆå¦‚æœé‡æ–°åŠ è½½ imageNet è¿™ç§ä»£ä»·å°±å¤ªå¤§äº†ï¼‰

### Overview

![](imgs/2.png)

### Drawback

how to train a generator è¿˜éœ€ç ”ç©¶ï¼Œç°åœ¨åœ¨ mnist ä¸Šæ•ˆæœè¿˜å¯ä»¥ï¼Œè¿™ä¸ªæ–¹å‘çš„å‘å±•å–å†³äºGANçš„å‘å±•ã€‚

## (3) Adding New Classes (é¡¹ç›®ç›¸å…³) (todo)

### Learning without Forgetting (ECCV 2016)

### iCaRL: Incremental Classifier and Representation Learning (CVPR 2017)



# 2. Knowledge Transfer: different from transfer learning

è¿ç§»å­¦ä¹ åªè€ƒè™‘å½“å‰ä»»åŠ¡çš„è¡¨ç°ï¼Œç»ˆèº«å­¦ä¹ è¿˜éœ€è¦ä¿è¯ä»¥å‰ä»»åŠ¡çš„è¡¨ç°ã€‚

## Example: Gradient Episodic Memory (GEM)

åœ¨GEMç®—æ³•ä¸­ï¼Œéœ€è¦ä¿ç•™éƒ¨åˆ†è¿‡å»çš„èµ„æ–™ï¼ˆsub dataset for previous tasksï¼‰

ç¼ºç‚¹ï¼šå¤§æ•°æ®é›†è¿˜æ˜¯ä¸è¡Œ

![](imgs/4.png)

![](imgs/5.png)

# 3. Model Expansion

## Progressive Neural Networks (2016)

æ¯ä¸€ä¸ªä»»åŠ¡éƒ½æœ‰è‡ªå·±çš„ç½‘ç»œï¼Œprevious taskçš„hidden layer outputä¹Ÿæ˜¯current taskçš„input

![](imgs/6.png)

## Expert Gate

è®­ç»ƒä¸€ä¸ªtask detectorï¼Œåˆ¤æ–­æ–°ä»»åŠ¡ä¸ previous tasks å“ªä¸ªæœ€åƒï¼Œç„¶åæŠŠé‚£ä¸ªä»»åŠ¡çš„ model å½“åšå½“å‰ä»»åŠ¡çš„ initializationã€‚

## Net2Net

å‰ä¸¤ç§æ–¹æ³•çš„ç¼ºç‚¹ï¼šä¸åŒçš„ä»»åŠ¡è¿˜æ˜¯ä¸åŒçš„æ¨¡å‹ã€‚

# Future Work: Curriculum Learning, ä»»åŠ¡è®­ç»ƒçš„é¡ºåºä¼šå½±å“æ¨¡å‹çš„è¡¨ç°

CVPR 2018 Best Paper â€”â€” ã€Štaskonomyã€‹