[TOC]

# 极大似然法——逻辑回归(logistic regression)的本质

极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，**利用试验结果得到某个参数值能够使样本出现的概率为最大**，则称为极大似然估计。

划重点：样本集中的样本都是独立同分布。

### 极大似然估计的例子——样本正态分布

![](https://img-blog.csdn.net/20170528003922141)

![](https://img-blog.csdn.net/20170528003926973)

![](https://img-blog.csdn.net/20170528004731774)

![](https://img-blog.csdn.net/20170528004738060)

似然方程有唯一解![img](https://img-blog.csdn.net/20170528004743185)

若似然函数满足连续可导的条件（如上例），则最大似然估计量就是如下方程的解。

### 极大似然估计的例子——样本均匀分布

![](https://img-blog.csdn.net/20170528005253964)

![](https://img-blog.csdn.net/20170528005304097)

很显然，L(a,b)作为a和b的二元函数是不连续的，这时不能用导数来求解。而必须从极大似然估计的定义出发，求L(a,b)的最大值，为使L(a,b)达到最大，b-a应该尽可能地小，但b又不能小于![img](https://img-blog.csdn.net/20170528005307589)，否则，L(a,b)=0。类似地a不能大过![img](https://img-blog.csdn.net/20170528005311058)，因此，a和b的极大似然估计：

![img](https://img-blog.csdn.net/20170528005314613)

# 逻辑回归的代价函数

如果利用误差平方和来当代价函数
$$
J(w) = \sum_{i} \dfrac{1}{2} (\phi(z^{(i)}) - y^{(i)})^2
$$
其中 $z^{(i)} = w^Tx^{(i)} + b$

将 sigmoid function 代入损失函数，得到的J是非凸函数，不行。

所以利用概率的思路：有了 Sigmoid function 之后，由于其取值在[0,1]，我们就可以将其视为**类1的后验概率估计p(y=1|x)**. 说白了，就是如果有了一个测试点x，那么就可以用 Sigmoid function 算出来的结果来当做该点x属于类别1的概率大小。
$$
p(y|x;w)=\phi(z)^{y}(1 - \phi(z))^{(1-y)}
$$
根据极大似然估计：
$$
L(w)=\prod_{i=1}^{n}p(y^{(i)}|x^{(i)};w)=\prod_{i=1}^{n}(\phi(z^{(i)}))^{y^{(i)}}(1-\phi(z^{(i)}))^{1-y^{(i)}}\\
l(w)=lnL(w)=\sum_{i = 1}^n y^{(i)}ln(\phi(z^{(i)})) + (1 - y^{(i)})ln(1-\phi(z^{(i)}))\\
J(w)=-l(w)=-\sum_{i = 1}^n y^{(i)}ln(\phi(z^{(i)})) + (1 - y^{(i)})ln(1-\phi(z^{(i)}))
$$

# 逻辑回归损失函数的梯度下降

$$
\dfrac{\partial J(w)}{w_j} = -\sum_{i=1}^n (y^{(i)}\dfrac{1}{\phi(z^{(i)})}-(1 - y^{(i)})\dfrac{1}{1-\phi(z^{(i)})})\dfrac{\partial \phi(z^{(i)})}{\partial w_j} \\ \quad   \quad \quad =-\sum_{i=1}^n (y^{(i)}\dfrac{1}{\phi(z^{(i)})}-(1 - y^{(i)})\dfrac{1}{1-\phi(z^{(i)})})\phi(z^{(i)})(1-\phi(z^{(i)}))\dfrac{\partial z^{(i)}}{\partial w_j} \\ \quad \quad \quad =-\sum_{i=1}^n (y^{(i)}(1-\phi(z^{(i)}))-(1-y^{(i)})\phi(z^{(i)}))x_j^{(i)} \\ \quad \quad \quad =-\sum_{i=1}^n (y^{(i)}-\phi(z^{(i)}))x_j^{(i)}
$$

所以梯度下降的公式：
$$
w_j :=w_j+\eta \sum_{i=1}^n (y^{(i)}-\phi(z^{(i)}))x_j^{(i)}
$$


# 逻辑回归的优缺点

### 优点

逻辑回归算法通过使用逻辑回归函数引入了非线性因素，因此可以轻松处理0/1分类问题。

（模型）模型清晰，背后的概率推导经得住推敲。
（输出）输出值自然地落在0到1之间，并且有概率意义
（参数）参数代表每个特征对输出的影响，可解释性强。
（简单高效）实施简单，非常高效（计算量小、存储占用低），可以在大数据场景中使用。
（可扩展）可以使用online learning的方式更新轻松更新参数，不需要重新训练整个模型。
（过拟合）解决过拟合的方法很多，如L1、L2正则化。
（多重共线性）L2正则化就可以解决多重共线性问题。

### 缺点

（特征相关情况）因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。
（特征空间）特征空间很大时，性能不好。
（精度）容易欠拟合，精度不高。

# 与决策树的比较

（数据的结构）逻辑回归胜在整体分析，决策树胜在局部分析。
（线性特性）逻辑回归擅长线性数据，决策树擅长非线性。
（缺失值）

