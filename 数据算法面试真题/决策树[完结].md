[TOC]

# 决策树的定义

在数据挖掘和机器学习训练中，决策树是有监督学习中一种常用的预测模型，这类决策树通常被称为回归决策树或分类决策树，预测连续值的就是回归树，预测离散值的就是分类树。注：决策树的工作原理其实很贴近人脑的思路，用图呈现出来的决策树工作流程其实就是树状图。

决策树就是从数据里分辨出关键特征，然后根据特征一步一步做预测，输出样本目标值。

# 如何构造一棵决策树 [todo]

在决策树构造过程中，选择什么属性作为根节点、子节点，什么时候停止到达叶节点是三个根本问题。

如何确定什么属性可以作为根节点、子节点和叶节点，依靠的概念是纯度、信息熵、信息增益，信息增益越大，纯度越低，我们要做的是将纯度最高的属性作为根节点。

> ID3 算法——以下内容来自：[https://www.jianshu.com/p/69dbb042a0e3](https://www.jianshu.com/p/69dbb042a0e3)

ID3 算法(Iterative Dichotomiser 3，迭代二叉树3代) 是一种贪心算法，用来构造决策树。ID3算法起源于概念学习系统（CLS），以**信息熵的下降速度（信息增益）**为选取测试属性的标准，即在每个节点选取还尚未被用来划分的具有最高信息增益的属性作为划分标准，然后继续这个过程，直到生成的决策树能完美分类训练样例。

理论依据是：信息熵越小，信息的纯度越高，也就是信息越少，在分类领域来讲就是里面包含的类别越少，所以我们可以得出，与初始信息熵的差越大分类效果越好。

```python
def calcShannonEnt(dataSet):
numEntries = len(dataSet) #数据集大小
labelCounts = {}
for featVec in dataSet:
    currentLabel = featVec[-1]   #获取分类标签
    if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0
    labelCounts[currentLabel] += 1  #每个类中数据个数统计
shannonEnt = 0.0
for key in labelCounts:  #信息熵计算
    prob = float(labelCounts[key])/numEntries
    shannonEnt -= prob * log(prob,2) 
return shannonEnt
```

```python
def chooseBestFeatureToSplit(dataSet):
numFeatures = len(dataSet[0]) - 1  #计算分类依据的个数
baseEntropy = calcShannonEnt(dataSet)   #计算原始分类的信息熵
bestInfoGain = 0.0; bestFeature = -1
for i in range(numFeatures):    #对apple进行分类
    featList = [example[i] for example in dataSet]
    uniqueVals = set(featList)
    newEntropy = 0.0
    for value in uniqueVals:  #计算该种分类的信息熵
        subDataSet = splitDataSet(dataSet, i, value)
        prob = len(subDataSet)/float(len(dataSet))
        newEntropy += prob * calcShannonEnt(subDataSet)     
    infoGain = baseEntropy - newEntropy  #计算当前分类的信息增益
    if (infoGain > bestInfoGain):  #比较那种分类的信息增益最大并返回
        bestInfoGain = infoGain
        bestFeature = i    
return bestFeature
```



# 决策树过拟合

### 过拟合的原因

##### 样本问题

1. 样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系；（什么是噪音数据？）
2. 样本抽取错误，包括（但不限于）样本数量太少，抽样方法错误，抽样时没有足够正确考虑业务场景或业务特点，等等导致抽出的样本数据不能有效足够代表业务逻辑或业务场景
3. 建模时使用了样本中太多无关的输入变量。

##### 构建决策树的方法问题

在决策树模型搭建中，我们使用的算法对于决策树的生长没有合理的限制和修剪的话，决策树的自由生长有可能每片叶子里只包含单纯的事件数据或非事件数据，可以想象，这种决策树当然可以完美匹配（拟合）训练数据，但是一旦应用到新的业务真实数据时，效果是一塌糊涂。

### 解决方案一：剪枝

分为先剪枝和后剪枝，用一定手段减少树杈分支，

> 剪枝——以下内容来自 [https://blog.csdn.net/qq_41951186/article/details/82779382](https://blog.csdn.net/qq_41951186/article/details/82779382)

##### 先剪枝prepruning

原则是——**限制决策树的高度和叶子结点处样本的数目**

1. 定义一个高度，当决策树达到该高度时就可以停止决策树的生长，这是一种最为简单的方法；
2. 达到某个结点的实例具有相同的特征向量，即使这些实例不属于同一类，也可以停止决策树的生长。这种方法对于处理数据中的数据冲突问题非常有效；
3. 定义一个阈值，当达到某个结点的实例个数小于该阈值时就可以停止决策树的生长；
4. 定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值的大小来决定是否停止决策树的生长。

##### 后剪枝postpruning

**REP方法**是一种比较简单的后剪枝的方法，在该方法中，可用的数据被分成两个样例集合：一个训练集用来形成学习到的决策树，一个分离的验证集用来评估这个决策树在后续数据上的精度，确切地说是用来评估修剪这个决策树的影响。这个方法的动机是：即使学习器可能会被训练集中的随机错误和巧合规律所误导，但验证集合不大可能表现出同样的随机波动。所以验证集可以用来对过度拟合训练集中的虚假特征提供防护检验。

1. 删除以此结点为根的子树使其成为叶子结点
2. 赋予该结点关联的训练数据的最常见分类
3. 当修剪后的树对于验证集合的性能不会比原来的树差时，才真正删除该结点

REP是最简单的后剪枝方法之一，不过由于使用独立的测试集，原始决策树相比，修改后的决策树可能偏向于过度修剪。这是因为一些不会再测试集中出现的很稀少的训练集实例所对应的分枝在剪枝过如果训练集较小，**通常不考虑采用REP算法**。尽管REP有这个缺点，不过REP仍然作为一种基准来评价其它剪枝算法的性能。它对于两阶段决策树学习方法的优点和缺点提供了了一个很好的学习思路。由于验证集合没有参与决策树的创建，所以用REP剪枝后的决策树对于测试样例的偏差要好很多，能够解决一定程度的过拟合问题。

---

**PEP方法**是根据剪枝前后的错误率来判定子树的修剪。该方法引入了统计学上连续修正的概念弥补REP中的缺陷，在评价子树的训练错误公式中添加了一个常数，假定每个叶子结点都自动对实例的某个部分进行错误的分类。它不需要像REP(错误率降低修剪)样，需要用部分样本作为测试数据，而是完全使用训练数据来生成决策树，又用这些训练数据来完成剪枝。决策树生成和剪枝都使用训练集, 所以会产生错分。

**把一棵子树（具有多个叶子节点）的分类用一个叶子节点来替代的话，在训练集上的误判率肯定是上升的，但是在测试数据上不一定，我们需要把子树的误判计算加上一个经验性的惩罚因子，用于估计它在测试数据上的误判率。**对于一棵叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为（E+0.5）/N。这个0.5就是惩罚因子。

那么对于该棵子树，假设它有L个叶子节点，则该子树的误判率估计为:

![img](https://img-blog.csdn.net/20171206194451321?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzIwNDM0OTU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

剪枝后该子树内部节点变成了叶子节点，该叶子结点的误判个数 J 同样也需要加上一个惩罚因子，变成 J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误 J+0.5 在

![img](https://img-blog.csdn.net/20171206194600979?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzIwNDM0OTU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

的标准误差内。对于样本的误差率e，我们可以根据经验把它估计成伯努利分布，那么可以估计出该子树的误判次数均值和标准差

![img](https://img-blog.csdn.net/20171206194911237?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzIwNDM0OTU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

 

使用训练数据，子树总是比替换为一个叶节点后产生的误差小，但是使用校正的误差计算方法却并非如此。剪枝的条件:当子树的误判个数大过对应叶节点的误判个数一个标准差之后，就决定剪枝：

![img](https://img-blog.csdn.net/20171206195241370?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzIwNDM0OTU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

 

这个条件就是剪枝的标准。当然并不一定非要大一个标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。

### 解决方案二：数据增强

# 优点

- 决策树易于理解和实现，同时决策树做决策的过程贴近人脑思维，因此通过解释人们很容易理解决策树表达的意义
- 效率高，决策树只需要一次构建，反复使用
- 决策树容易实现可视化，使结果更加直观易懂

# 缺点

- 容易过度拟合
- 不适合处理高维数据
- 对异常值（outlier）比较敏感，容易导致树的结构发生改变